TfPilot: Replace Polling with SSE + Event-Driven Sync (Cheapest Scalable Path)
Goals

Instant status updates across:

Requests table

Request detail page

Templates / create request UI (if relevant)

Any page showing request status

Reduce GitHub API calls drastically:

No more per-client polling of /sync

Only sync on demand (user action) or on real events (webhooks)

Preserve invariants:

No optimistic status writes

Status derived only (deriveLifecycleStatus(request))

Sync patches request doc (never replaces)

UI reads derived status only

Minimal safe changes, easy rollback

Non-goals

Real-time collaborative editing

Streaming logs/terminal (future)

Multi-region, multi-tenant event bus (future)

Current State (baseline)

List page polls /api/requests every 30s

Detail page polls /api/requests/[id]/sync adaptively

Sync endpoint calls GitHub multiple times, patches S3 doc, UI derives status from facts 

PLATFORM_FULL_AUDIT_REPORT

Rate-aware GET wrapper reduces some GitHub load but polling still fans out per user/tab 

PLATFORM_FULL_AUDIT_REPORT

Target State (what we’re building)
A) One server→client stream for updates: SSE

Add a single SSE endpoint that:

Holds an HTTP connection open

Pushes events when request facts change

Lets the UI update instantly without polling

B) Event-driven sync: GitHub webhooks

Instead of clients “asking” for updates:

GitHub workflow/run/PR events trigger a server-side sync

Sync patches S3 doc

Server emits SSE event → clients update instantly

C) “Fallback mode”

Keep minimal polling as safety net:

Low-frequency list polling (e.g., 2–5 min) or “manual refresh”

Detail polling only if SSE disconnected

Allows safe rollout

Why SSE (not WebSockets) for TfPilot
SSE fits:

Traffic is mostly server → client

No need for bidirectional messaging

Cheap infra (no redis/pubsub required initially)

Simpler behind ALB/Cloudflare

Easy to implement in Next.js API routes

Easier operationally (no connection state machine like WS)

WebSockets requires:

Connection management

Multi-instance fanout (usually Redis pubsub)

Heartbeats, reconnect logic, backpressure

Higher complexity without extra product value

Key Design Decision: “Facts updated → emit event”

TfPilot status is derived from facts (PR, runs, approval, etc.). 

PLATFORM_FULL_AUDIT_REPORT

So the event system should be driven by:

“request doc patched” (facts changed)
not by:

“status changed” (status is derived and can change due to derivation rules)

Event payload should carry facts minimal summary + derived status for UI convenience, but fact changes remain source of truth.

Event Model
Event types (minimal set)

request.updated

Emitted when request doc is patched and any relevant fields changed

Includes a summary: requestId, updatedAt, derivedStatus, key facts (pr state, run statuses)

request.deleted / request.archived (optional)

If you archive requests into history and want table to update instantly

system.notice (optional)

For rate limit warnings, degraded mode, etc.

Event payload (shape)

eventId: monotonic or timestamp+random

type: as above

ts: server timestamp

requestId

patchSummary: “what changed” (optional; helps debugging)

requestSummary:

derivedStatus (server derived for convenience)

updatedAt

pr: { number, state, merged }

planRun/applyRun/destroyRun: { status, conclusion, runId, updatedAt }

approval: { approvedBy, approvedAt } (if applicable)

Keep it small. Don’t send full plan output, diffs, logs.

SSE Endpoint Design
Endpoint

GET /api/events (or /api/requests/events)

Auth

Requires session cookie (same as other routes)

If not authed: return 401 and do not open stream

Subscription scope

Start simple:

One global stream per user

Server pushes all updates user is allowed to see (since your system is internal)

Optional later:

?requestId= filter for detail pages

?projects=core,payments etc.

Connection behavior

Send initial hello event

Send keepalive comment/event periodically (to avoid idle timeouts)

Support reconnection:

SSE supports Last-Event-ID

But you can also keep it simple: on reconnect, UI re-fetches list once

Multi-instance caveat

If you run multiple Next.js instances (ECS):

SSE connections will attach to different instances

An event emitted on instance A won’t reach connections on instance B unless you fan out

Cheapest path:

Start with in-memory fanout (works in single instance)

Add Redis pubsub only when you truly need scale

Event Emitter / Fanout Layer
Phase 1: In-memory hub (fastest + cheapest)

A singleton “EventHub” in server memory:

subscribe(userId) → stream

publish(event) → broadcast to subscribers

Works if:

Single instance

Or you accept “best effort” updates (fallback polling covers misses)

Phase 2: Redis pubsub (scale-ready)

Publisher publishes to Redis channel

Each instance subscribes, forwards to its local SSE clients

Choose Redis only when:

You have >1 instance and require guaranteed instant updates

Triggering Updates (the big win)
Today

Client polling drives sync → sync calls GitHub.

Target

Server receives events → server runs sync → emits request.updated.

There are 3 sources of events:

1) User-initiated mutations (immediate)

When user clicks:

plan / apply / destroy / approve / merge / update config

After the mutation route patches request doc:

emit request.updated immediately (or emit “mutation started” + “patched”)

This makes UI update instantly without waiting for polling.

2) GitHub webhooks (the main scaling win)

Subscribe to:

workflow_run (completed, requested)

pull_request (opened, synchronize, closed)

pull_request_review (submitted)

check_suite / check_run (optional)

On receiving webhook:

identify requestId (via branch name, PR title marker, workflow input, or requestId label)

call internal sync for that requestId (server-side, not user token)

patch S3 doc

emit request.updated

This eliminates most GitHub API calls triggered by polling.

3) Periodic reconciliation (optional safety net)

A very low-frequency job (e.g., every 10–30 min) to:

scan recently updated requests

sync only active ones

ensures recovery if a webhook was missed

This is cheap and protects you from webhook delivery gaps.

The Hard Part: Server-side GitHub auth for webhooks

Your current GitHub calls rely on user OAuth tokens in session cookies. 

PLATFORM_FULL_AUDIT_REPORT

Webhooks are server-driven. You need a server credential.

Cheapest options:

Option A (recommended): GitHub App installation token

Best practice for server automation

Stable

Fine-grained permissions

Avoids storing long-lived PAT

Option B: Fine-scoped PAT (fastest, less ideal)

A single PAT stored as secret

Scopes: repo read + workflow read

Works now, but not as clean

Given you’re already operating as a platform: GitHub App is the right long-term play. But if speed matters: do PAT first then migrate.

Reducing GitHub API calls further

Even with webhooks, sync still fetches from GitHub. Improve by:

Only sync on meaningful webhook types

Ignore noise (workflow_run requested vs completed, etc.)

Prefer webhook payload facts

Some webhook payloads already include status/conclusion/PR state

You can patch minimal facts directly and defer “full sync” to later

But keep it safe: don’t regress correctness

Tighten run correlation

Always validate run head_sha / branch mapping (you already do this for apply) 

PLATFORM_FULL_AUDIT_REPORT

Same for destroy to avoid wrong run attachment

UI Changes
Global Event Subscription

Create a client-side hook/provider:

starts SSE connection once

keeps latest “request summaries” in memory (store)

notifies pages to revalidate or patch local state

Requests table

Instead of polling:

fetch once on load (/api/requests)

apply SSE updates to the row(s) instantly

Request detail page

Instead of polling /sync:

fetch once on load

listen for request.updated for that requestId

update UI instantly

Fallback behavior

If SSE fails/disconnects:

fallback to current polling but at a slower interval

show small “reconnecting…” toast/badge

Rollout Plan (safe + reversible)
Phase 1 — SSE only (no webhooks yet)

Add SSE endpoint + in-memory EventHub

Emit events from mutation routes + sync route

UI subscribes and updates

Keep polling as fallback (long interval)

Result: “instant” updates for user actions, fewer sync polls.

Phase 2 — GitHub webhooks

Add webhook endpoint

Verify signatures

Map webhook → requestId

Trigger server-side sync

Emit events

Result: remove most polling, status updates become truly reactive.

Phase 3 — Multi-instance fanout

Add Redis pubsub (optional)

Or accept best-effort SSE + low-frequency reconciling

Risks / Tradeoffs

Multi-instance SSE misses events

Mitigation: fallback list refresh occasionally + optional Redis later

Webhook delivery reliability

Mitigation: reconciliation job + webhook delivery logging

Auth model change

Moving from user token to app token requires careful permissions

Keeping invariants

Never “set status” directly; only patch facts and derive

Success Metrics

GitHub API calls/day down by >70%

Avg request page update latency: <1–2s

Sync endpoint called mostly by webhooks, not clients

No correctness regressions in lifecycle derivation